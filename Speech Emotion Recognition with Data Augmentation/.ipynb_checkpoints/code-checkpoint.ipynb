{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5be1040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os, glob\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe7ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for extracting the features\n",
    "def extract_feature(file_name):\n",
    "    X, sample_rate = librosa.load(os.path.join(file_name), res_type='kaiser_best')\n",
    "    stft=np.abs(librosa.stft(X))\n",
    "    result=np.array([])\n",
    "    mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "    result=np.hstack((result,mfccs))\n",
    "    chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result=np.hstack((result,chroma))\n",
    "    spectrogram=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "    result=np.hstack((result,spectrogram))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fdaaa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for matching filenames with emotion\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "observed_emotions={'calm':0, 'happy':1, 'fearful':2, 'disgust':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f65817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for loading the data\n",
    "def load_data(filepath): \n",
    "    x,g,y,f=[],[],[],[]\n",
    "    \n",
    "    # feature to extract\n",
    "    mfcc = True\n",
    "\n",
    "    paths = []\n",
    "    paths.append(filepath)\n",
    "   \n",
    "    for path in paths:\n",
    "        for file in glob.glob(path):\n",
    "            file_name=os.path.basename(file)\n",
    "            emotion=emotions[file_name.split(\"-\")[2]] #to get emotion according to filename. dictionary emotions is defined above.\n",
    "            if emotion not in observed_emotions.keys():\n",
    "                continue\n",
    "            if(int(file_name.split(\"-\")[6].split(\".\")[0])%2):\n",
    "                g.append(\"Male\")\n",
    "            else:\n",
    "                g.append(\"Female\")\n",
    "            feature=extract_feature(file)\n",
    "            x.append(feature)\n",
    "            y.append(emotion)\n",
    "            f.append(file_name)\n",
    "    return {\"X\":x,\"g\":g,\"y\":y,\"f\":f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c646ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "def data_processing():\n",
    "    start_time = time.time()\n",
    "    Trial_dict = load_data(\"C:/Users/david/Downloads/archive/Actor_*/*.wav\")\n",
    "    print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))\n",
    "    #converting to dataframe\n",
    "    X = pd.DataFrame(Trial_dict[\"X\"])\n",
    "    g = pd.DataFrame(Trial_dict[\"g\"])\n",
    "    y = pd.DataFrame(Trial_dict[\"y\"])\n",
    "    f = pd.DataFrame(Trial_dict[\"f\"])\n",
    "    #renaming column names\n",
    "    y=y.rename(columns= {0: 'emotion'})\n",
    "    g=g.rename(columns= {0: 'gender'})\n",
    "    f=f.rename(columns= {0: 'filename'})\n",
    "    #concatenating the dataframes\n",
    "    data = pd.concat([X, f, g, y], axis =1)\n",
    "    #for making the data random\n",
    "    # data = data.reindex(np.random.permutation(data.index))\n",
    "    #saving data to csv\n",
    "    data.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f79adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded. Loading time: 41.67772436141968 seconds ---\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  data = pd.read_csv(\"data.csv\")\n",
    "except:\n",
    "  data_processing()\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c334cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-728.250427</td>\n",
       "      <td>63.978127</td>\n",
       "      <td>1.503606</td>\n",
       "      <td>16.672989</td>\n",
       "      <td>6.537086</td>\n",
       "      <td>3.505902</td>\n",
       "      <td>-4.833341</td>\n",
       "      <td>-2.226708</td>\n",
       "      <td>-11.258503</td>\n",
       "      <td>-3.375772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7.595593e-07</td>\n",
       "      <td>6.779792e-08</td>\n",
       "      <td>1.358367e-09</td>\n",
       "      <td>03-01-02-01-01-01-01.wav</td>\n",
       "      <td>Male</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-708.444824</td>\n",
       "      <td>68.275627</td>\n",
       "      <td>0.720012</td>\n",
       "      <td>12.693843</td>\n",
       "      <td>8.231519</td>\n",
       "      <td>2.551562</td>\n",
       "      <td>-5.898926</td>\n",
       "      <td>-3.235186</td>\n",
       "      <td>-9.733943</td>\n",
       "      <td>-3.424062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.485640e-06</td>\n",
       "      <td>1.162135e-07</td>\n",
       "      <td>2.015163e-09</td>\n",
       "      <td>03-01-02-01-01-02-01.wav</td>\n",
       "      <td>Male</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-698.417786</td>\n",
       "      <td>66.880730</td>\n",
       "      <td>-0.836123</td>\n",
       "      <td>15.390104</td>\n",
       "      <td>4.217039</td>\n",
       "      <td>4.619611</td>\n",
       "      <td>-3.044673</td>\n",
       "      <td>-3.098406</td>\n",
       "      <td>-10.319806</td>\n",
       "      <td>-2.937279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>2.374604e-05</td>\n",
       "      <td>1.855033e-06</td>\n",
       "      <td>1.632362e-08</td>\n",
       "      <td>03-01-02-01-02-01-01.wav</td>\n",
       "      <td>Male</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-699.818726</td>\n",
       "      <td>70.371246</td>\n",
       "      <td>-0.566691</td>\n",
       "      <td>15.343685</td>\n",
       "      <td>6.694068</td>\n",
       "      <td>3.695070</td>\n",
       "      <td>-2.224451</td>\n",
       "      <td>-4.148477</td>\n",
       "      <td>-9.418019</td>\n",
       "      <td>-2.885618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>5.528448e-06</td>\n",
       "      <td>5.825631e-07</td>\n",
       "      <td>1.021049e-08</td>\n",
       "      <td>03-01-02-01-02-02-01.wav</td>\n",
       "      <td>Male</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-735.483826</td>\n",
       "      <td>72.453140</td>\n",
       "      <td>2.309139</td>\n",
       "      <td>15.769185</td>\n",
       "      <td>7.388165</td>\n",
       "      <td>3.709811</td>\n",
       "      <td>-5.551935</td>\n",
       "      <td>-1.069268</td>\n",
       "      <td>-8.113977</td>\n",
       "      <td>-2.737980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8.572507e-07</td>\n",
       "      <td>1.085423e-07</td>\n",
       "      <td>1.208249e-09</td>\n",
       "      <td>03-01-02-02-01-01-01.wav</td>\n",
       "      <td>Male</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>-569.106567</td>\n",
       "      <td>52.730045</td>\n",
       "      <td>-19.179628</td>\n",
       "      <td>7.030930</td>\n",
       "      <td>-13.469654</td>\n",
       "      <td>-10.614804</td>\n",
       "      <td>-15.646835</td>\n",
       "      <td>-14.295638</td>\n",
       "      <td>-15.181238</td>\n",
       "      <td>-1.810298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>6.542808e-05</td>\n",
       "      <td>1.065850e-05</td>\n",
       "      <td>2.307115e-07</td>\n",
       "      <td>03-01-07-01-02-02-24.wav</td>\n",
       "      <td>Female</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-609.862061</td>\n",
       "      <td>55.874176</td>\n",
       "      <td>-12.270983</td>\n",
       "      <td>0.797629</td>\n",
       "      <td>-19.799801</td>\n",
       "      <td>-6.321277</td>\n",
       "      <td>-13.306941</td>\n",
       "      <td>-14.632303</td>\n",
       "      <td>-11.185981</td>\n",
       "      <td>1.281417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.633628e-06</td>\n",
       "      <td>4.461181e-07</td>\n",
       "      <td>9.269638e-09</td>\n",
       "      <td>03-01-07-02-01-01-24.wav</td>\n",
       "      <td>Female</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>-504.026733</td>\n",
       "      <td>33.492260</td>\n",
       "      <td>-25.259033</td>\n",
       "      <td>-0.905644</td>\n",
       "      <td>-17.759993</td>\n",
       "      <td>-8.268425</td>\n",
       "      <td>-17.365608</td>\n",
       "      <td>-9.657678</td>\n",
       "      <td>-6.430394</td>\n",
       "      <td>0.995612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>8.167539e-05</td>\n",
       "      <td>7.214972e-06</td>\n",
       "      <td>9.843457e-08</td>\n",
       "      <td>03-01-07-02-01-02-24.wav</td>\n",
       "      <td>Female</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-511.855103</td>\n",
       "      <td>33.078693</td>\n",
       "      <td>-25.351046</td>\n",
       "      <td>-3.669427</td>\n",
       "      <td>-17.204227</td>\n",
       "      <td>-9.845312</td>\n",
       "      <td>-16.486992</td>\n",
       "      <td>-13.139003</td>\n",
       "      <td>-7.608793</td>\n",
       "      <td>-1.511128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>2.639075e-04</td>\n",
       "      <td>3.939355e-05</td>\n",
       "      <td>6.879179e-07</td>\n",
       "      <td>03-01-07-02-02-01-24.wav</td>\n",
       "      <td>Female</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>-517.092957</td>\n",
       "      <td>28.806524</td>\n",
       "      <td>-19.843636</td>\n",
       "      <td>0.777045</td>\n",
       "      <td>-15.487811</td>\n",
       "      <td>-10.588390</td>\n",
       "      <td>-19.108301</td>\n",
       "      <td>-12.121792</td>\n",
       "      <td>-9.705316</td>\n",
       "      <td>-1.216895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>2.874513e-04</td>\n",
       "      <td>2.767977e-05</td>\n",
       "      <td>2.821950e-07</td>\n",
       "      <td>03-01-07-02-02-02-24.wav</td>\n",
       "      <td>Female</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5  \\\n",
       "0   -728.250427  63.978127   1.503606  16.672989   6.537086   3.505902   \n",
       "1   -708.444824  68.275627   0.720012  12.693843   8.231519   2.551562   \n",
       "2   -698.417786  66.880730  -0.836123  15.390104   4.217039   4.619611   \n",
       "3   -699.818726  70.371246  -0.566691  15.343685   6.694068   3.695070   \n",
       "4   -735.483826  72.453140   2.309139  15.769185   7.388165   3.709811   \n",
       "..          ...        ...        ...        ...        ...        ...   \n",
       "763 -569.106567  52.730045 -19.179628   7.030930 -13.469654 -10.614804   \n",
       "764 -609.862061  55.874176 -12.270983   0.797629 -19.799801  -6.321277   \n",
       "765 -504.026733  33.492260 -25.259033  -0.905644 -17.759993  -8.268425   \n",
       "766 -511.855103  33.078693 -25.351046  -3.669427 -17.204227  -9.845312   \n",
       "767 -517.092957  28.806524 -19.843636   0.777045 -15.487811 -10.588390   \n",
       "\n",
       "             6          7          8         9  ...       173       174  \\\n",
       "0    -4.833341  -2.226708 -11.258503 -3.375772  ...  0.000008  0.000004   \n",
       "1    -5.898926  -3.235186  -9.733943 -3.424062  ...  0.000003  0.000003   \n",
       "2    -3.044673  -3.098406 -10.319806 -2.937279  ...  0.000014  0.000034   \n",
       "3    -2.224451  -4.148477  -9.418019 -2.885618  ...  0.000020  0.000024   \n",
       "4    -5.551935  -1.069268  -8.113977 -2.737980  ...  0.000001  0.000002   \n",
       "..         ...        ...        ...       ...  ...       ...       ...   \n",
       "763 -15.646835 -14.295638 -15.181238 -1.810298  ...  0.000685  0.000445   \n",
       "764 -13.306941 -14.632303 -11.185981  1.281417  ...  0.000033  0.000024   \n",
       "765 -17.365608  -9.657678  -6.430394  0.995612  ...  0.000471  0.000421   \n",
       "766 -16.486992 -13.139003  -7.608793 -1.511128  ...  0.002634  0.002444   \n",
       "767 -19.108301 -12.121792  -9.705316 -1.216895  ...  0.002864  0.002467   \n",
       "\n",
       "          175       176           177           178           179  \\\n",
       "0    0.000002  0.000002  7.595593e-07  6.779792e-08  1.358367e-09   \n",
       "1    0.000004  0.000003  1.485640e-06  1.162135e-07  2.015163e-09   \n",
       "2    0.000052  0.000054  2.374604e-05  1.855033e-06  1.632362e-08   \n",
       "3    0.000024  0.000015  5.528448e-06  5.825631e-07  1.021049e-08   \n",
       "4    0.000004  0.000003  8.572507e-07  1.085423e-07  1.208249e-09   \n",
       "..        ...       ...           ...           ...           ...   \n",
       "763  0.000253  0.000126  6.542808e-05  1.065850e-05  2.307115e-07   \n",
       "764  0.000016  0.000014  4.633628e-06  4.461181e-07  9.269638e-09   \n",
       "765  0.000366  0.000234  8.167539e-05  7.214972e-06  9.843457e-08   \n",
       "766  0.001352  0.000727  2.639075e-04  3.939355e-05  6.879179e-07   \n",
       "767  0.001515  0.001025  2.874513e-04  2.767977e-05  2.821950e-07   \n",
       "\n",
       "                     filename  gender  emotion  \n",
       "0    03-01-02-01-01-01-01.wav    Male     calm  \n",
       "1    03-01-02-01-01-02-01.wav    Male     calm  \n",
       "2    03-01-02-01-02-01-01.wav    Male     calm  \n",
       "3    03-01-02-01-02-02-01.wav    Male     calm  \n",
       "4    03-01-02-02-01-01-01.wav    Male     calm  \n",
       "..                        ...     ...      ...  \n",
       "763  03-01-07-01-02-02-24.wav  Female  disgust  \n",
       "764  03-01-07-02-01-01-24.wav  Female  disgust  \n",
       "765  03-01-07-02-01-02-24.wav  Female  disgust  \n",
       "766  03-01-07-02-02-01-24.wav  Female  disgust  \n",
       "767  03-01-07-02-02-02-24.wav  Female  disgust  \n",
       "\n",
       "[768 rows x 183 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee34a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_result(clf,clf_name,X_train, y_train,X_test,y_test):\n",
    "    print(\"\\n\"+clf_name+\": \\n\")\n",
    "    starting_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Trained model in %s ms \" % str(time.time() - starting_time))\n",
    "    y_train_pred= clf.predict(X_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    #printing classification report, accuracy and confusion matrix\n",
    "    print(classification_report(y_test,y_pred))\n",
    "\n",
    "    acc = float(accuracy_score(y_test,y_pred))*100\n",
    "    train_acc=float(accuracy_score(y_train,y_train_pred))*100\n",
    "    print(\"----Training accuracy score %s ----\" % train_acc)\n",
    "    print(\"----Accuracy score %s ----\" % acc)\n",
    "    train_acc_dict[clf_name]=train_acc\n",
    "    test_acc_dict[clf_name]=acc\n",
    "    #cm = confusion_matrix(y_test,y_pred)\n",
    "    #df_cm = pd.DataFrame(cm)\n",
    "    #sns.heatmap(df_cm, annot=True, fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e824370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a random forest classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def randomforest(X_train, y_train,X_test,y_test):\n",
    "    clf = RandomForestClassifier(n_estimators=200,criterion='gini',max_features='sqrt',max_depth=15, random_state=0)\n",
    "    classify_result(clf,\"Random Forest\",X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d79a6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=X_train.to_numpy()\n",
    "# y_train=y_train.to_numpy()\n",
    "# y_train=y_train.flatten()\n",
    "# X_test=X_test.to_numpy()\n",
    "# y_test=y_test.to_numpy()\n",
    "# y_test=y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ea5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "# import torch\n",
    "# clf = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "#                        optimizer_params=dict(lr=1e-3),\n",
    "#                        scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
    "#                        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "#                        mask_type='entmax' ) \n",
    "# clf.fit(\n",
    "#   X_train, y_train,eval_metric=['accuracy'],\n",
    "#                max_epochs=1000 , patience=50,\n",
    "#                batch_size=28, drop_last=False\n",
    "# )\n",
    "# y_train_pred= clf.predict(X_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# #printing classification report, accuracy and confusion matrix\n",
    "# print(classification_report(y_test,y_pred))\n",
    "\n",
    "# acc = float(accuracy_score(y_test,y_pred))*100\n",
    "# train_acc=float(accuracy_score(y_train,y_train_pred))*100\n",
    "# print(\"----Training accuracy score %s ----\" % train_acc)\n",
    "# print(\"----Accuracy score %s ----\" % acc)\n",
    "# train_acc_dict[\"Tabnet\"]=train_acc\n",
    "# test_acc_dict[\"Tabnet\"]=acc\n",
    "# cm = confusion_matrix(y_test,y_pred)\n",
    "# df_cm = pd.DataFrame(cm)\n",
    "# sns.heatmap(df_cm, annot=True, fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec237a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "def MLP(X_train, y_train,X_test,y_test):\n",
    "    clf = MLPClassifier(alpha=0.01, batch_size=256,epsilon=1e-08,hidden_layer_sizes=(300,),learning_rate='adaptive',max_iter=500,random_state=0)\n",
    "    classify_result(clf,\"Multi Layer Perceptron\",X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b07120c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def KNN(X_train, y_train,X_test,y_test):\n",
    "    clf = KNeighborsClassifier(n_neighbors=5,weights='distance')\n",
    "    classify_result(clf,\"K Neighbors\",X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab9e8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def SVM(X_train, y_train,X_test,y_test):\n",
    "    clf =SVC(kernel=\"linear\", C=0.025)\n",
    "    classify_result(clf,\"Support Vector Machine\",X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5315e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "def GaussianProcess(X_train, y_train,X_test,y_test):\n",
    "    kernel = 1.0 * RBF(1.0)\n",
    "\n",
    "    clf = GaussianProcessClassifier(kernel=kernel,random_state=0,multi_class='one_vs_one')\n",
    "    classify_result(clf,\"Gaussian Process Classifier\",X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e15f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "def XGB(X_train, y_train,X_test,y_test):\n",
    "    clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss',eta=0.1)\n",
    "    classify_result(clf,\"XG Boost\",X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8df95aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(data):\n",
    "    #data=data.replace({\"emotion\": observed_emotions})\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    data['emotion']= label_encoder.fit_transform(data['emotion'])\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    data['gender']= label_encoder.fit_transform(data['gender'])\n",
    "    \n",
    "    #splitting to features and target variables\n",
    "    X=data.drop([\"emotion\",\"gender\",\"filename\"],axis=1)\n",
    "    y=data.iloc[:,-1:]\n",
    "    \n",
    "    #splitting into test and train sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # from sklearn.preprocessing import MinMaxScaler\n",
    "    # scaler = MinMaxScaler()\n",
    "    # X_train = scaler.fit_transform(X_train)\n",
    "    # X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # mean = np.mean(X_train, axis=0)\n",
    "    # std = np.std(X_train, axis=0)\n",
    "    # X_train = (X_train - mean)/std\n",
    "    # X_test = (X_test - mean)/std\n",
    "    \n",
    "    randomforest(X_train, y_train,X_test,y_test)\n",
    "    MLP(X_train, y_train,X_test,y_test)\n",
    "    KNN(X_train, y_train,X_test,y_test)\n",
    "    SVM(X_train, y_train,X_test,y_test)\n",
    "    GaussianProcess(X_train, y_train,X_test,y_test)\n",
    "    XGB(X_train, y_train,X_test,y_test)\n",
    "    print(\"Training accuracy: \",train_acc_dict)\n",
    "    print(\"Testing accuracy: \",test_acc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71c7236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest: \n",
      "\n",
      "Trained model in 0.6293962001800537 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83        39\n",
      "           1       0.62      0.68      0.65        38\n",
      "           2       0.75      0.54      0.63        39\n",
      "           3       0.67      0.63      0.65        38\n",
      "\n",
      "    accuracy                           0.69       154\n",
      "   macro avg       0.70      0.69      0.69       154\n",
      "weighted avg       0.70      0.69      0.69       154\n",
      "\n",
      "----Training accuracy score 100.0 ----\n",
      "----Accuracy score 69.48051948051948 ----\n",
      "\n",
      "Multi Layer Perceptron: \n",
      "\n",
      "Trained model in 1.5426957607269287 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.85        39\n",
      "           1       0.77      0.79      0.78        38\n",
      "           2       0.71      0.74      0.72        39\n",
      "           3       0.62      0.66      0.64        38\n",
      "\n",
      "    accuracy                           0.75       154\n",
      "   macro avg       0.75      0.75      0.75       154\n",
      "weighted avg       0.75      0.75      0.75       154\n",
      "\n",
      "----Training accuracy score 100.0 ----\n",
      "----Accuracy score 74.67532467532467 ----\n",
      "\n",
      "K Neighbors: \n",
      "\n",
      "Trained model in 0.0 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90        39\n",
      "           1       0.75      0.79      0.77        38\n",
      "           2       0.68      0.54      0.60        39\n",
      "           3       0.65      0.68      0.67        38\n",
      "\n",
      "    accuracy                           0.74       154\n",
      "   macro avg       0.73      0.74      0.73       154\n",
      "weighted avg       0.73      0.74      0.73       154\n",
      "\n",
      "----Training accuracy score 100.0 ----\n",
      "----Accuracy score 74.02597402597402 ----\n",
      "\n",
      "Support Vector Machine: \n",
      "\n",
      "Trained model in 0.015599250793457031 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77        39\n",
      "           1       0.75      0.63      0.69        38\n",
      "           2       0.59      0.59      0.59        39\n",
      "           3       0.51      0.53      0.52        38\n",
      "\n",
      "    accuracy                           0.64       154\n",
      "   macro avg       0.64      0.64      0.64       154\n",
      "weighted avg       0.65      0.64      0.64       154\n",
      "\n",
      "----Training accuracy score 77.68729641693811 ----\n",
      "----Accuracy score 64.28571428571429 ----\n",
      "\n",
      "Gaussian Process Classifier: \n",
      "\n",
      "Trained model in 2.7096781730651855 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.74      0.81        39\n",
      "           1       0.90      0.71      0.79        38\n",
      "           2       0.63      0.74      0.68        39\n",
      "           3       0.60      0.71      0.65        38\n",
      "\n",
      "    accuracy                           0.73       154\n",
      "   macro avg       0.75      0.73      0.73       154\n",
      "weighted avg       0.75      0.73      0.73       154\n",
      "\n",
      "----Training accuracy score 98.37133550488599 ----\n",
      "----Accuracy score 72.72727272727273 ----\n",
      "\n",
      "XG Boost: \n",
      "\n",
      "Trained model in 0.6660592555999756 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82        39\n",
      "           1       0.74      0.68      0.71        38\n",
      "           2       0.70      0.72      0.71        39\n",
      "           3       0.57      0.53      0.55        38\n",
      "\n",
      "    accuracy                           0.70       154\n",
      "   macro avg       0.70      0.70      0.70       154\n",
      "weighted avg       0.70      0.70      0.70       154\n",
      "\n",
      "----Training accuracy score 100.0 ----\n",
      "----Accuracy score 70.12987012987013 ----\n",
      "Training accuracy:  {'Random Forest': 100.0, 'Multi Layer Perceptron': 100.0, 'K Neighbors': 100.0, 'Support Vector Machine': 77.68729641693811, 'Gaussian Process Classifier': 98.37133550488599, 'XG Boost': 100.0}\n",
      "Testing accuracy:  {'Random Forest': 69.48051948051948, 'Multi Layer Perceptron': 74.67532467532467, 'K Neighbors': 74.02597402597402, 'Support Vector Machine': 64.28571428571429, 'Gaussian Process Classifier': 72.72727272727273, 'XG Boost': 70.12987012987013}\n"
     ]
    }
   ],
   "source": [
    "train_acc_dict={}\n",
    "test_acc_dict={}\n",
    "modelling(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "605f5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "def add_white_noise(signal, noise_percentage_factor):\n",
    "    noise = np.random.normal(0, signal.std(), signal.size)\n",
    "    augmented_signal = signal + noise * noise_percentage_factor\n",
    "    return augmented_signal\n",
    "\n",
    "\n",
    "def time_stretch(signal, time_stretch_rate):\n",
    "    \"\"\"Time stretching implemented with librosa:\n",
    "    https://librosa.org/doc/main/generated/librosa.effects.pitch_shift.html?highlight=pitch%20shift#librosa.effects.pitch_shift\n",
    "    \"\"\"\n",
    "    return librosa.effects.time_stretch(signal, time_stretch_rate)\n",
    "\n",
    "\n",
    "def pitch_scale(signal, sr, num_semitones):\n",
    "    \"\"\"Pitch scaling implemented with librosa:\n",
    "    https://librosa.org/doc/main/generated/librosa.effects.pitch_shift.html?highlight=pitch%20shift#librosa.effects.pitch_shift\n",
    "    \"\"\"\n",
    "    return librosa.effects.pitch_shift(signal, sr, num_semitones)\n",
    "\n",
    "\n",
    "def random_gain(signal, min_factor=0.5, max_factor=2):\n",
    "    #gain_rate = random.uniform(min_factor, max_factor)\n",
    "    gain_rate = min_factor\n",
    "    augmented_signal = signal * gain_rate\n",
    "    return augmented_signal\n",
    "\n",
    "\n",
    "def invert_polarity(signal):\n",
    "    return signal * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08270662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmenting(train_filename_list):\n",
    "    paths = []\n",
    "    paths.append(\"C:/Users/david/Downloads/archive/Actor_*/*.wav\")\n",
    "   \n",
    "    for path in paths:\n",
    "        for file in glob.glob(path):\n",
    "            file_name = os.path.basename(file)\n",
    "            if file_name not in train_filename_list:\n",
    "                continue\n",
    "            emotion = emotions[file_name.split(\"-\")[2]] #to get emotion according to filename. dictionary emotions is defined above.\n",
    "            if emotion not in observed_emotions.keys():\n",
    "                continue\n",
    "            X, sample_rate = librosa.load(os.path.join(file), res_type='kaiser_best')\n",
    "            augmented_signal = add_white_noise(X, 0.1)\n",
    "            augmented_signal = time_stretch(augmented_signal, 0.5)\n",
    "            #augmented_signal = pitch_scale(augmented_signal, sample_rate, 2)\n",
    "            augmented_signal = random_gain(augmented_signal)\n",
    "            augmented_signal = invert_polarity(augmented_signal)\n",
    "            sf.write(\"Augmented/augmented\"+file_name, augmented_signal, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da625bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_data_processing():\n",
    "    start_time = time.time()\n",
    "    Trial_dict = load_data(\"C:/Users/david/Downloads/dissertation/Augmented/*.wav\")\n",
    "    print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))\n",
    "    #converting to dataframe\n",
    "    X = pd.DataFrame(Trial_dict[\"X\"])\n",
    "    g = pd.DataFrame(Trial_dict[\"g\"])\n",
    "    y = pd.DataFrame(Trial_dict[\"y\"])\n",
    "    #renaming column names\n",
    "    y=y.rename(columns= {0: 'emotion'})\n",
    "    g=g.rename(columns= {0: 'gender'})\n",
    "    #concatenating the dataframes\n",
    "    augmented_data = pd.concat([X, g, y], axis =1)\n",
    "    #for making the data random\n",
    "    # augmented_data = augmented_data.reindex(np.random.permutation(data.index))\n",
    "    #saving data to csv\n",
    "    augmented_data.to_csv('augmented_data.csv', index=False)\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f65b7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#   augmented_data = pd.read_csv(\"augmented_data.csv\")\n",
    "# except:\n",
    "#   augmented_data_processing()\n",
    "\n",
    "# augmented_data = pd.read_csv(\"augmented_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86ed09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_modelling():\n",
    "    data = pd.read_csv(\"data.csv\")\n",
    "    \n",
    "    #data=data.replace({\"emotion\": observed_emotions})\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    label_encoder_emotion = preprocessing.LabelEncoder()\n",
    "    data['emotion']= label_encoder_emotion.fit_transform(data['emotion'])\n",
    "    label_encoder_gender = preprocessing.LabelEncoder()\n",
    "    data['gender']= label_encoder_gender.fit_transform(data['gender'])\n",
    "    \n",
    "    #splitting to features and target variables\n",
    "    X=data.drop([\"emotion\",\"gender\"],axis=1)\n",
    "    y=data.iloc[:,-1:]\n",
    "    \n",
    "    #splitting into test and train sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    train_filename_list = X_train[\"filename\"].values.tolist()\n",
    "    X_train = X_train.drop(\"filename\",axis=1)\n",
    "    X_test = X_test.drop(\"filename\",axis=1)\n",
    "    \n",
    "    augmenting(train_filename_list)\n",
    "    augmented_data = augmented_data_processing()\n",
    "    \n",
    "    augmented_data['emotion']= label_encoder_emotion.transform(augmented_data['emotion'])\n",
    "    \n",
    "    augmented_data['gender']= label_encoder_gender.transform(augmented_data['gender']) \n",
    "    \n",
    "    X_augmented=augmented_data.drop([\"emotion\",\"gender\"],axis=1)\n",
    "    y_augmented=augmented_data.iloc[:,-1:]\n",
    "\n",
    "    X_augmented.columns=X_train.columns\n",
    "    X_train = pd.concat([X_train, X_augmented])\n",
    "    y_train = pd.concat([y_train, y_augmented])\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # from sklearn.preprocessing import MinMaxScaler\n",
    "    # scaler = MinMaxScaler()\n",
    "    # X_train = scaler.fit_transform(X_train)\n",
    "    # X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # mean = np.mean(X_train, axis=0)\n",
    "    # std = np.std(X_train, axis=0)\n",
    "    # X_train = (X_train - mean)/std\n",
    "    # X_test = (X_test - mean)/std\n",
    "    \n",
    "    randomforest(X_train, y_train,X_test,y_test)\n",
    "    MLP(X_train, y_train,X_test,y_test)\n",
    "    KNN(X_train, y_train,X_test,y_test)\n",
    "    SVM(X_train, y_train,X_test,y_test)\n",
    "    GaussianProcess(X_train, y_train,X_test,y_test)\n",
    "    XGB(X_train, y_train,X_test,y_test)\n",
    "    print(\"Training accuracy: \",train_acc_dict)\n",
    "    print(\"Testing accuracy: \",test_acc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0580a410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded. Loading time: 30.52586817741394 seconds ---\n",
      "\n",
      "Random Forest: \n",
      "\n",
      "Trained model in 1.2572824954986572 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83        39\n",
      "           1       0.53      0.61      0.57        38\n",
      "           2       0.63      0.56      0.59        39\n",
      "           3       0.61      0.53      0.56        38\n",
      "\n",
      "    accuracy                           0.64       154\n",
      "   macro avg       0.64      0.64      0.64       154\n",
      "weighted avg       0.64      0.64      0.64       154\n",
      "\n",
      "----Training accuracy score 100.0 ----\n",
      "----Accuracy score 64.28571428571429 ----\n",
      "\n",
      "Multi Layer Perceptron: \n",
      "\n",
      "Trained model in 3.1260504722595215 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.89        39\n",
      "           1       0.78      0.76      0.77        38\n",
      "           2       0.62      0.72      0.67        39\n",
      "           3       0.66      0.71      0.68        38\n",
      "\n",
      "    accuracy                           0.75       154\n",
      "   macro avg       0.77      0.75      0.75       154\n",
      "weighted avg       0.77      0.75      0.75       154\n",
      "\n",
      "----Training accuracy score 100.0 ----\n",
      "----Accuracy score 74.67532467532467 ----\n",
      "\n",
      "K Neighbors: \n",
      "\n",
      "Trained model in 0.015619039535522461 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90        39\n",
      "           1       0.70      0.82      0.76        38\n",
      "           2       0.65      0.56      0.60        39\n",
      "           3       0.63      0.58      0.60        38\n",
      "\n",
      "    accuracy                           0.72       154\n",
      "   macro avg       0.71      0.72      0.72       154\n",
      "weighted avg       0.72      0.72      0.72       154\n",
      "\n",
      "----Training accuracy score 100.0 ----\n",
      "----Accuracy score 72.07792207792207 ----\n",
      "\n",
      "Support Vector Machine: \n",
      "\n",
      "Trained model in 0.06289148330688477 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.80        39\n",
      "           1       0.69      0.66      0.68        38\n",
      "           2       0.62      0.62      0.62        39\n",
      "           3       0.56      0.53      0.54        38\n",
      "\n",
      "    accuracy                           0.66       154\n",
      "   macro avg       0.66      0.66      0.66       154\n",
      "weighted avg       0.66      0.66      0.66       154\n",
      "\n",
      "----Training accuracy score 75.1628664495114 ----\n",
      "----Accuracy score 66.23376623376623 ----\n",
      "\n",
      "Gaussian Process Classifier: \n",
      "\n",
      "Trained model in 10.95285415649414 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        39\n",
      "           1       0.64      0.76      0.70        38\n",
      "           2       0.62      0.26      0.36        39\n",
      "           3       0.33      0.82      0.47        38\n",
      "\n",
      "    accuracy                           0.45       154\n",
      "   macro avg       0.40      0.46      0.38       154\n",
      "weighted avg       0.40      0.45      0.38       154\n",
      "\n",
      "----Training accuracy score 59.853420195439746 ----\n",
      "----Accuracy score 45.45454545454545 ----\n",
      "\n",
      "XG Boost: \n",
      "\n",
      "Trained model in 1.4837515354156494 ms \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.85        39\n",
      "           1       0.73      0.63      0.68        38\n",
      "           2       0.68      0.69      0.68        39\n",
      "           3       0.69      0.63      0.66        38\n",
      "\n",
      "    accuracy                           0.72       154\n",
      "   macro avg       0.72      0.72      0.72       154\n",
      "weighted avg       0.72      0.72      0.72       154\n",
      "\n",
      "----Training accuracy score 100.0 ----\n",
      "----Accuracy score 72.07792207792207 ----\n",
      "Training accuracy:  {'Random Forest': 100.0, 'Multi Layer Perceptron': 100.0, 'K Neighbors': 100.0, 'Support Vector Machine': 75.1628664495114, 'Gaussian Process Classifier': 59.853420195439746, 'XG Boost': 100.0}\n",
      "Testing accuracy:  {'Random Forest': 64.28571428571429, 'Multi Layer Perceptron': 74.67532467532467, 'K Neighbors': 72.07792207792207, 'Support Vector Machine': 66.23376623376623, 'Gaussian Process Classifier': 45.45454545454545, 'XG Boost': 72.07792207792207}\n"
     ]
    }
   ],
   "source": [
    "train_acc_dict={}\n",
    "test_acc_dict={}\n",
    "augmented_modelling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
